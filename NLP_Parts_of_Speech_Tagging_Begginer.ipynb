{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Parts-of-Speech-Tagging-Begginer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQ6BqFinMnzTRakrxFluT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-alamdari/NLP-Parts-of-Speech-Tagging-Begginer/blob/main/NLP_Parts_of_Speech_Tagging_Begginer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TrZXsr_1HMnT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/amanjeetsahu/Natural-Language-Processing-Specialization/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%202/WSJ_24.pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYFdEwFvHYZj",
        "outputId": "7315eea7-4c53-414b-fc0b-fcdccaadfae6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-15 12:00:45--  https://raw.githubusercontent.com/amanjeetsahu/Natural-Language-Processing-Specialization/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%202/WSJ_24.pos\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 286063 (279K) [text/plain]\n",
            "Saving to: ‘WSJ_24.pos.1’\n",
            "\n",
            "\rWSJ_24.pos.1          0%[                    ]       0  --.-KB/s               \rWSJ_24.pos.1        100%[===================>] 279.36K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-04-15 12:00:45 (12.4 MB/s) - ‘WSJ_24.pos.1’ saved [286063/286063]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/amanjeetsahu/Natural-Language-Processing-Specialization/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%202/WSJ_02-21.pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8FVO2vXIJcx",
        "outputId": "fbd857c1-ab79-4bab-eecd-2465d13f2ed9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-15 12:00:46--  https://raw.githubusercontent.com/amanjeetsahu/Natural-Language-Processing-Specialization/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%202/WSJ_02-21.pos\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8279089 (7.9M) [text/plain]\n",
            "Saving to: ‘WSJ_02-21.pos.1’\n",
            "\n",
            "\rWSJ_02-21.pos.1       0%[                    ]       0  --.-KB/s               \rWSJ_02-21.pos.1     100%[===================>]   7.90M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-04-15 12:00:46 (115 MB/s) - ‘WSJ_02-21.pos.1’ saved [8279089/8279089]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/amanjeetsahu/Natural-Language-Processing-Specialization/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%202/hmm_vocab.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY1AzKe5Iati",
        "outputId": "3add38f3-f6b6-4333-aa48-caec804bfff9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-15 12:03:04--  https://raw.githubusercontent.com/amanjeetsahu/Natural-Language-Processing-Specialization/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%202/hmm_vocab.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 196571 (192K) [text/plain]\n",
            "Saving to: ‘hmm_vocab.txt’\n",
            "\n",
            "hmm_vocab.txt       100%[===================>] 191.96K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-04-15 12:03:04 (10.4 MB/s) - ‘hmm_vocab.txt’ saved [196571/196571]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('WSJ_02-21.pos', 'r') as f:\n",
        "  train_corpus = f.readlines()"
      ],
      "metadata": {
        "id": "CavVoovII8XA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus[10: 20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS2weJQDKE_c",
        "outputId": "5a532853-ae0e-4f55-94e6-877773e89abf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['at\\tIN\\n',\n",
              " 'Chicago\\tNNP\\n',\n",
              " \"'s\\tPOS\\n\",\n",
              " 'Goodman\\tNNP\\n',\n",
              " 'Theatre\\tNNP\\n',\n",
              " '(\\t(\\n',\n",
              " '``\\t``\\n',\n",
              " 'Revitalized\\tVBN\\n',\n",
              " 'Classics\\tNNS\\n',\n",
              " 'Take\\tVBP\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('WSJ_24.pos', 'r') as f:\n",
        "  y = f.readlines()"
      ],
      "metadata": {
        "id": "a_zJ20e5KHo9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[10: 20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaUTRDeDKVAX",
        "outputId": "c284347b-fa68-4094-d331-b40f9aff5ab6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['points\\tNNS\\n',\n",
              " 'this\\tDT\\n',\n",
              " 'week\\tNN\\n',\n",
              " ',\\t,\\n',\n",
              " 'with\\tIN\\n',\n",
              " 'readings\\tNNS\\n',\n",
              " 'on\\tIN\\n',\n",
              " 'trade\\tNN\\n',\n",
              " ',\\t,\\n',\n",
              " 'output\\tNN\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('hmm_vocab.txt', 'r') as f:\n",
        "  vocabulary_list = f.read().split('\\n')\n",
        "\n",
        "vocab = {}\n",
        "for ind, word in enumerate(sorted(vocabulary_list)):\n",
        "  vocab[word] = ind"
      ],
      "metadata": {
        "id": "2o-Bvo-NKWL2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(vocabulary_list)[10: 20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDmdlb89I7r8",
        "outputId": "d91c1e59-1541-4790-ecbd-7edaebb5cd92"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\"]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for id, (k, v) in enumerate(vocab.items()):\n",
        "  print(k, v)\n",
        "  if id == 10:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCAwzGojI74W",
        "outputId": "75e057f8-dfa9-432e-b130-81d3b929aa62"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0\n",
            "! 1\n",
            "# 2\n",
            "$ 3\n",
            "% 4\n",
            "& 5\n",
            "' 6\n",
            "'' 7\n",
            "'40s 8\n",
            "'60s 9\n",
            "'70s 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%202/utils_pos.py\n",
        "\n",
        "import string\n",
        "\n",
        "\n",
        "# Punctuation characters\n",
        "punct = set(string.punctuation)\n",
        "\n",
        "# Morphology rules used to assign unknown word tokens\n",
        "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
        "\n",
        "\n",
        "def get_word_tag(line, vocab): \n",
        "    if not line.split():\n",
        "        word = \"--n--\"\n",
        "        tag = \"--s--\"\n",
        "        return word, tag\n",
        "    else:\n",
        "        word, tag = line.split()\n",
        "        if word not in vocab: \n",
        "            # Handle unknown words\n",
        "            word = assign_unk(word)\n",
        "        return word, tag\n",
        "    return None \n",
        "\n",
        "\n",
        "def assign_unk(tok):\n",
        "    \"\"\"\n",
        "    Assign unknown word tokens\n",
        "    \"\"\"\n",
        "    # Digits\n",
        "    if any(char.isdigit() for char in tok):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Punctuation\n",
        "    elif any(char in punct for char in tok):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Upper-case\n",
        "    elif any(char.isupper() for char in tok):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Nouns\n",
        "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Verbs\n",
        "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Adjectives\n",
        "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Adverbs\n",
        "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"--unk_adv--\"\n",
        "\n",
        "    return \"--unk--\"\n",
        "\n",
        "def preprocess(vocab, data_fp):\n",
        "    \"\"\"\n",
        "    Preprocess data\n",
        "    \"\"\"\n",
        "    orig = []\n",
        "    prep = []\n",
        "\n",
        "    with open(data_fp, \"r\") as data_file:\n",
        "        for cnt, word in enumerate(data_file):\n",
        "            # End of sentence\n",
        "            if not word.split():\n",
        "                orig.append(word.strip())\n",
        "                word = \"--n--\"\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            # Handle unknown words\n",
        "            elif word.strip() not in vocab:\n",
        "                orig.append(word.strip())\n",
        "                word = assign_unk(word)\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                orig.append(word.strip())\n",
        "                prep.append(word.strip())\n",
        "\n",
        "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
        "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
        "\n",
        "    return orig, prep\n"
      ],
      "metadata": {
        "id": "a1GOoR6hJJ34"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/amanjeetsahu/Natural-Language-Processing-Specialization/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%202/test.words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcQocgb7P1mT",
        "outputId": "e0908cd0-d134-45e1-cb0a-d17f839fcf63"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-15 12:43:47--  https://raw.githubusercontent.com/amanjeetsahu/Natural-Language-Processing-Specialization/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%202/test.words\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180266 (176K) [text/plain]\n",
            "Saving to: ‘test.words’\n",
            "\n",
            "test.words          100%[===================>] 176.04K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-04-15 12:43:47 (11.6 MB/s) - ‘test.words’ saved [180266/180266]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, prep = preprocess(vocab, 'test.words')"
      ],
      "metadata": {
        "id": "UyTgNoipQD8n"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep[10:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUnZI7VSQKu0",
        "outputId": "6bf25cc1-4889-405d-899f-fe1db5c8c687"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['points',\n",
              " 'this',\n",
              " 'week',\n",
              " ',',\n",
              " 'with',\n",
              " 'readings',\n",
              " 'on',\n",
              " 'trade',\n",
              " ',',\n",
              " 'output']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O9cSGiTNQNNU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}